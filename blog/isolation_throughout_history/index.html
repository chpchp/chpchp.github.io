<!DOCTYPE html>
<html>
<head>
  <title>Isolation Throughout History</title>
  <link href='/blog/css/style.css' rel='stylesheet'>
  <link href='style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="/blog">Blog</a>
  </div>

  <div id=container>
    <h1>Isolation Throughout History</h1>
    <p>
    Concurrency in a relational database is a bit like coffee. Too little and
    things run a bit sluggish. Too much and things get a little crazy.
    Typically, databases offer a range of concurrency options (from decaf to
    espresso) which all make a particular trade-off between allowing
    transactions to run more concurrently and guaranteeing that transactions
    run with fewer anomalies. These various levels of concurrency are called
    <strong>isolation levels</strong>, and they can be particularly tricky to
    understand. In fact, they are so tricky that researchers have been trying
    to define them for over 40 years! In this article, we'll take a walk
    through time and explore various definitions of isolation from 1977 to
    2017.
    </p>

    <h2>1977: Degrees of Consistency</h2>
    <p>
    In 1977, Gray et al. took a first stab at defining various isolation levels
    which they called <strong>degrees of consistency</strong>. Here are the
    definitions verbatim:
    </p>

    <ul>
      <li>
        <strong>Degree 0:</strong> Transaction $T$ sees degree 0 consistency if
        <ol type="a">
          <li>$T$ does not overwrite dirty data of other transactions.</li>
        </ol>
      </li>

      <li>
        <strong>Degree 1:</strong> Transaction $T$ sees degree 1 consistency if
        <ol type="a">
          <li>$T$ does not overwrite dirty data of other transactions.</li>
          <li>$T$ does not commit any writes before EOT.</li>
        </ol>
      </li>

      <li>
        <strong>Degree 2:</strong> Transaction $T$ sees degree 2 consistency if
        <ol type="a">
          <li>$T$ does not overwrite dirty data of other transactions.</li>
          <li>$T$ does not commit any writes before EOT.</li>
          <li>$T$ does not read dirty data of other transactions.</li>
        </ol>
      </li>

      <li>
        <strong>Degree 3:</strong> Transaction $T$ sees degree 3 consistency if
        <ol type="a">
          <li>$T$ does not overwrite dirty data of other transactions.</li>
          <li>$T$ does not commit any writes before EOT.</li>
          <li>$T$ does not read dirty data of other transactions.</li>
          <li>Other transactions do not dirty any data read by $T$ before $T$
              completes.</li>
        </ol>
      </li>
    </ul>

    <p>
    Now, let's walk through each degree of consistency, describe it in a bit
    more detail, and give some intuition as to what kinds of anomalies each
    degree of consistency prevents.
    </p>

    <ul>
      <li>
        Degree 0 is a bit weird because it assumes that a transaction $T$ can
        commit a write before $T$ itself commits. This degree is obsolete,
        since we no longer make this assumption.
      </li>
      <li>
        Degree 1 forbids dirty writes. Once a transaction $T$ <em>writes</em>
        to $x$, no other transaction can <em>write</em> to $x$ until $T$
        commits or aborts. Assume we have the invariant $x = y$. Without
        degree 1 consistency, the invariant could be broken by the following
        history: $W_1(x=42)$, $W_2(x=9001)$, $W_2(y=9001)$, $W_1(y=42)$.
      </li>
      <li>
        Degree 2 forbids dirty reads. Once a transaction $T$ <em>writes</em> to
        $x$, no other transaction can <em>read</em> $x$ until $T$ commits or
        aborts.  Imagine again that we have the invariant $x = y$. In the
        following history, $T_2$ is able to read an inconsistent state:
        $W_1(x=42)$, $R_2(x=42)$, $R_2(y=9001)$, $W_1(x=42)$.
      </li>
      <li>
        Degree 3 forbids unrepeatable reads. Once a transaction $T$
        <em>reads</em> $x$, no other transaction can <em>write</em> to $x$.
        Without degree 3, the following history could occur in which $T_1$
        reads different values of $x$: $R_1(x=42)$, $W_2(x=9001)$, $C_2$,
        $R_1(x=9001)$. Degree 3 may also forbid the phantom problem, but
        unfortunately its definition is a bit too vague to determine.
      </li>
    </ul>

    <p>
    Note that Gray et al.'s definitions apply to a single transaction, not to a
    history, but the definitions extend naturally to histories. If all
    transactions in a history see degree $i$ consistency, then we say the
    history is degree $i$ consistent.
    </p>

    <p>
    Gray et al. also define one locking protocol for every degree of
    consistency. The degree $i$ locking protocol is guaranteed to only produce
    histories that are degree $i$ consistent:
    </p>

    <ul>
      <li><strong>Degree 0 Locking</strong>: transactions acquire short-lived
          write locks.</li>
      <li><strong>Degree 1 Locking</strong>: transactions acquire long-lived
          write locks.</li>
      <li><strong>Degree 2 Locking</strong>: transactions acquire long-lived
          write locks and short-lived read locks.</li>
      <li><strong>Degree 3 Locking</strong>: transactions acquire long-lived
          write locks and long-lived read locks.</li>
    </ul>

    <h2>1992: ANSI SQL</h2>
    <p>
    The 1992 ANSI SQL standard provided their own definitions of various
    isolation levels. Each isolation level was defined using a set of
    <strong>phenomena</strong>; stronger isolation levels prevented more
    phenomena. Like Gray et al.'s 1977 definitions, the ANSI SQL standard's
    definitions are written in English and expectedly vague.
    </p>

    <p>
    Unfortunately, the 1992 ANSI SQL standard
    <a href="https://goo.gl/NUjW1b">costs a whopping $60</a>, so I can't state
    the definitions verbatim. However, Berenson et al.'s critique provides a
    summary of the definitions which I include verbatim here (they may actually
    be verbatim; I have no way of checking):
    </p>
    <ul>
      <li>
        <strong>P1 (Dirty Read)</strong>: Transaction $T_1$ modifies a data
        item. Another transaction $T_2$ then reads that data item before $T_1$
        performs a <code>COMMIT</code> or <code>ROLLBACK</code>. If $T_1$ then
        performs a <code>ROLLBACK</code>, $T_2$ has read a data item that was
        never committed and so never really existed.
      </li>
      <li>
        <strong>P2 (Non-repeatable or Fuzzy Read)</strong>: Transaction $T_1$
        reads a data item.  Another transaction $T_2$ then modifies or deletes
        that data item and commits. If $T_1$ then attempts to reread the data
        item, it receives a modified value or discovers that the data item has
        been deleted.
      </li>
      <li>
        <strong>P3 (Phantom)</strong>: Transaction $T_1$ reads a set of data
        items satisfying some <code>&lt;search condition&gt;</code>.
        Transaction $T_2$ then creates data items that satisfy $T_1$'s
        <code>&lt;search condition&gt;</code> and commits. If $T_1$ then
        repeats its read with the same <code>&lt;search condition&gt;</code>,
        it gets a set of data items different from the first read.
      </li>
    </ul>

    <p>
    The 1992 ANSI SQL standard then defined a set of isolation levels using
    these phenomena.
    </p>

    <ul>
      <li><strong>ANSI READ UNCOMMITTED</strong> histories permit all
          anomalies.</li>
      <li><strong>ANSI READ COMMITTED</strong> histories are those without
          P1.</li>
      <li><strong>ANSI REPEATABLE READ</strong> histories are those without P1
          or P2.</li>
      <li><strong>ANSI ANOMALY SERIALIZABLE</strong> histories are those
          without P1, P2, or P3.</li>
    </ul>

    <p>
    Note that with a pedantic interpretation of these definitions, ANOMALY
    SERIALIZABLE is not the same as SERIALIZABLE. We'll touch more on this in
    the next section.
    </p>

    <h2>1995: A Critique</h2>
    <p>
    In 1995, Berenson et al. published a critique of the 1992 ANSI SQL
    standard's definitions of isolation levels. The definitions were
    incoherent, incorrect, and incomplete.
    </p>

    <h3>Formalizing Anomalies</h3>
    <p>
    Berenson et al. first formalized the English definitions of the phenomena
    in the ANSI standard as a set of subhistories. In doing so, they note that
    the English definitions can be interpreted rather strictly or interpreted a
    bit more loosely. Interpreted strictly:
    </p>

    <ul>
      <li>
        <strong>A1</strong>: $W_1(x)$, $\ldots$, $R_2(x)$, $\ldots$, ($A_1$ and
        $C_2$ in either order).
      </li>
      <li>
        <strong>A2</strong>: $R_1(x)$, $\ldots$, $W_2(x)$, $\ldots$, $C_2$,
        $\ldots$, $R_1(x)$, $\ldots$, $C_1$.
      </li>
      <li>
        <strong>A3</strong>: $R_1(P)$, $\ldots$, $W_2(y\ \text{in}\ P)$,
        $\ldots$, $C_2$, $\ldots$, $R_1(P)$, $\ldots$, $C_1$.
      </li>
    </ul>

    <p>
    Interpreted loosely:
    </p>

    <ul>
      <li>
        <strong>P1</strong>: $W_1(x)$, $\ldots$, $R_2(x)$, $\ldots$, (($C_1$ or
        $A_1$) and ($C_2$ or $A_2$) in any order).
      </li>
      <li>
        <strong>P2</strong>: $R_1(x)$, $\ldots$, $W_2(x)$, $\ldots$, (($C_1$ or
        $A_1$) and ($C_2$ or $A_2$) in any order).
      </li>
      <li>
        <strong>P3</strong>: $R_1(P)$, $\ldots$, $W_2(y\ \text{in}\ P)$,
        $\ldots$, (($C_1$ or $A_1$) and ($C_2$ or $A_2$) in any order).
      </li>
    </ul>

    <p>
    Note that the loose interpretation corresponds to the anomalies described
    by Gray et al. in 1977.
    </p>

    <h3>Revisiting Lock Protocols</h3>
    <p>
    Berenson et al. then refined Gray et al.'s locking protocols.
    </p>

    <ul>
      <li><strong>Degree 0 Locking</strong>: No read locks, no predicate locks,
          short-lived write locks.</li>
      <li><strong>Degree 1 Locking = Locking READ UNCOMMITTED</strong>: No read
          locks, no predicate locks, long-lived write locks.</li>
      <li><strong>Degree 2 Locking = Locking READ COMMITTED</strong>:
          short-lived read locks, short-lived predicate locks, long-lived write
          locks.</li>
      <li><strong>Locking REPEATABLE READ</strong>: long-lived read locks,
          short-lived predicate locks, long-lived write locks.</li>
      <li><strong>Degree 3 Locking = Locking SERIALIZABLE</strong>: long-lived
          read locks, long-lived predicate locks, long-lived write locks.</li>
    </ul>

    <h3>The Critique</h3>
    <p>
    Berenson et al. then begin their critique of the 1992 ANSI SQL standard.
    First, they point out that the standard does not discuss blind writes at
    all! READ UNCOMMITTED through ANOMALY SERIALIZABLE all allow blind writes.
    What a flub!
    </p>

    <p>
    Next, Berenson et al. point out that the strict interpretations of the
    standard's phenomena are incomplete. They still permit a number of
    anomalies that do not occur with the looser interpretations.
    </p>

    <ul>
      <li>
        <strong>P1 over A1.</strong> Consider the invariant $x = y$ and the
        history $W_1(x=42)$, $R_1(x=42)$, $R_2(y=9001)$, $C_2$, $W_1(y=42)$,
        $C_1$. $T_2$ observes a state that violates the invariant, but the
        history does not violate any A1, A2, or A3. It does, however, violate
        P1.
      </li>
      <li>
        <strong>P2 over A2.</strong> Consider the invariant $x = y$ and the
        history $R_1(x=42)$, $W_2(x=9001)$, $W_3(y=9001)$, $C_2$,
        $R_1(y=9001)$, $C_1$. $T_2$ observes a state that violates the
        invariant, but the history does not violate any of P1, A2, or A3. It
        does, however, violate P2.
      </li>
      <li>
        <strong>P3 over A3.</strong> Consider the invariant that $z$ is equal
        to the number of entries returned by the predicate $P$. The history
        $R_1(P)$, $W_2(\text{insert}\ y\ \text{into}\ P)$, $R_2(z)$, $W_2(z)$,
        $C_2$, $R_1(z)$, $C_1$ satisfies P1, P2, and A3, but $T_1$ reads a
        state which violates the invariant. It does, however, violate P3.
      </li>
    </ul>

    <h3>Cursor Stability</h3>
    <p>
    Next, Berenson et al. introduce a new anomaly called lost update:
    </p>

    <ul>
      <li>
        <strong>P4 (Lost Update)</strong>: $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, $W_1(x)$, $\ldots$, $C_1$.
      </li>
    </ul>

    <p>
    This phenomenon allows $T_1$ to clobber $T_2$'s write. For example,
    consider the following history in which $T_1$ and $T_2$ both attempt to
    increment a counter $x$ by 1: $R_1(x=0)$, $R_2(x=0)$, $W_2(x=1)$, $C_2$,
    $W_2(x=1)$, $C_1$. Due to the lost update, only one of the increments is
    processed.
    </p>

    <p>
    Histories which prohibit anomalies P0, P1, and P4 satisfy the
    <strong>cursor stability</strong> isolation level. Cursor stability is more
    restrictive than read committed but less restrictive than repeatable read.
    </p>

    <p>
    The Locking READ COMMITTED protocol acquires short-lived read locks. With a
    cursor stability locking protocol, a transaction acquires a read lock on an
    object and only releases the lock if it will not subsequently write to it.
    Otherwise, it upgrades the read lock to a write lock and holds the write
    lock until commit or abort.
    </p>

    <h3>Snapshot Isolation</h3>
    <p>
    Berenson et al. then introduce <strong>snapshot isolation</strong>. Under
    snapshot isolation, every transaction reads from a snapshot of the database
    taken when the transaction starts. A transaction does not read the writes
    of other concurrently running transactions.
    </p>

    <p>
    Snapshot isolation is typically implemented using multiversion concurrency
    control. When a transaction begins, it is assigned a start timestamp. When
    a transaction commits, it is assigned a commit timestamp that is higher
    than any other start or commit timestamp. If the transaction's writes do
    not conflict with any other write committed between its start and commit
    timestamp, the transaction commits.
    </p>

    <p>
    To characterize snapshot isolation, we introduce two new anomalies:
    </p>

    <ul>
      <li>
        <strong>A5A (Read Skew):</strong> $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, $W_2(y)$, $\ldots$, $C_2$, $\ldots$, $R_1(y)$, $\ldots$,
        ($C_1$ or $A_1$).
      </li>
      <li>
        <strong>A5B (Write Skew):</strong> $R_1(x)$, $\ldots$, $R_2(y)$,
        $\ldots$, $W_1(y)$, $\ldots$, $W_2(x)$, $\ldots$, ($C_1$ and $C_2$).
      </li>
    </ul>

    <p>
    Consider the invariant $x = y$. The history $R_1(x=42)$, $W_2(x=9001)$,
    $W_2(y=9001)$, $C_2$, $R_2(y=9001)$, $C_2$ suffers from read skew, and
    $T_1$ reads an inconsistent state.
    </p>

    <p>
    Consider the invariant $x + y \geq 0$. The history $R_1(x=1)$, $R_1(y=1)$,
    $R_2(x=1)$, $R_2(y=1)$, $W_1(x=-1)$, $W_2(y=-1)$, $C_1$, $C_2$ suffers from
    write skew and leaves the database in an inconsistent state.
    </p>

    <p>
    Snapshot isolation is stronger than read committed, weaker than full
    serializability, and incomparable to repeatable read. Understanding this
    precisely is a bit difficult because snapshot isolation uses multiversion
    concurrency control, and thus far we have only considered single version
    histories (more on this in the next section). Loosely speaking though,
    snapshot isolation prevents P0, P1, A2, A3, P4, and A5A. Repeatable read
    prevents P0, P1, P2, P4, A5A, and A5B.
    </p>

    <h3>Summary</h3>
    <ul>
      <li>
        <strong>P0 (Dirty Write)</strong>: $W_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, (($C_1$ or $A_1$) and ($C_2$ or $A_2$)).
      </li>
      <li>
        <strong>A1 (Dirty Read)</strong>: $W_1(x)$, $\ldots$, $R_2(x)$,
        $\ldots$, ($A_1$ and $C_2$).
      </li>
      <li>
        <strong>A2 (Sneaky Write)</strong>: $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, $C_2$, $\ldots$, $R_1(x)$, $\ldots$, $C_1$.
      </li>
      <li>
        <strong>A3 (Phantom)</strong>: $R_1(P)$, $\ldots$, $W_2(y\ \text{in}\
        P)$, $\ldots$, $C_2$, $\ldots$, $R_1(P)$, $\ldots$, $C_1$.
      </li>
      <li>
        <strong>P1 (Dirty Read)</strong>: $W_1(x)$, $\ldots$, $R_2(x)$,
        $\ldots$, (($C_1$ or $A_1$) and ($C_2$ or $A_2$)).
      </li>
      <li>
        <strong>P2 (Sneaky Write)</strong>: $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, (($C_1$ or $A_1$) and ($C_2$ or $A_2$)).
      </li>
      <li>
        <strong>P3 (Phantom)</strong>: $R_1(P)$, $\ldots$, $W_2(y\ \text{in}\
        P)$, $\ldots$, (($C_1$ or $A_1$) and ($C_2$ or $A_2$)).
      </li>
      <li>
        <strong>P4 (Lost Update)</strong>: $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, $W_1(x)$, $\ldots$, $C_1$.
      </li>
      <li>
        <strong>A5A (Read Skew):</strong> $R_1(x)$, $\ldots$, $W_2(x)$,
        $\ldots$, $W_2(y)$, $\ldots$, $C_2$, $\ldots$, $R_1(y)$, $\ldots$,
        ($C_1$ or $A_1$).
      </li>
      <li>
        <strong>A5B (Write Skew):</strong> $R_1(x)$, $\ldots$, $R_2(y)$,
        $\ldots$, $W_1(y)$, $\ldots$, $W_2(x)$, $\ldots$, ($C_1$ and $C_2$).
      </li>
    </ul>

    <center>
      <table>
        <thead>
          <tr>
            <th></th>
            <th>P0</th>
            <th>A0</th>
            <th>A1</th>
            <th>A2</th>
            <th>P0</th>
            <th>P1</th>
            <th>P2</th>
            <th>P4</th>
            <th>A5A</th>
            <th>A5B</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>RU</td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
          </tr>
          <tr>
            <td>RC</td>
            <td>x</td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
            <td> </td>
          </tr>
          <tr>
            <td>CS</td>
            <td>x</td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td>x</td>
            <td> </td>
            <td> </td>
          </tr>
          <tr>
            <td>RR</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td> </td>
            <td>x</td>
            <td>x</td>
            <td> </td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
          </tr>
          <tr>
            <td>SI</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td> </td>
            <td> </td>
            <td>x</td>
            <td>x</td>
            <td> </td>
          </tr>
          <tr>
            <td>S</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
            <td>x</td>
          </tr>
        </tbody>
      </table>
    </center>

    <h2>2000: Generalized Isolation Levels</h2>
    <p>
    In 2000, Adya et al. pointed out the limitations of Berenson's et al.'s
    definitions:
    </p>

    <h3>The Shortcomings of Berenson et al</h3>
    <ul>
      <li>
        The definitions are overly restrictive. For example, the history
        $W_1(x)$, $W_2(x)$, $C_1$, $C_2$ is disallowed by P0 even though the
        history is serializable.
      </li>
      <li>
        The definitions were specific to single-versioned locking protocols.
        Berenson et al' say things like "P0, P1, P2, and P3 are disguised
        redefinitions of locking behavior" and that "Snapshot Isolation is a
        multi-version (MV) method, so single-valued (SV) histories do not
        properly reflect the temporal operations sequences."
      </li>
      <li>
        The definitions place restrictions on transactions that abort. Some
        optimistic concurrency control techniques are permissive in what they
        allow pending transactions to do and make sure to abort them later if
        it would create an anomaly.
      </li>
      <li>
        The definitions dot not treat predicates very formally.
      </li>
    </ul>

    <h3>Database Model</h3>
    <p>TODO</p>

    <h3>Generalized Isolation Levels</h3>
    <p>
    Adya et al.' defined a new set of generalized anomalies:
    </p>

    <ul>
      <li>
        <strong>G0: Write Cycles.</strong> A history $H$ exbits phenomenon G0
        if DSG($H$) contains a directed cycle consisting entirely of
        write-dependency edges.
      </li>
      <li>
        <strong>G1a: Aborted Reads.</strong> A history $H$ shows phenomenon G1a
        if it contains an aborted transaction $T_1$ and a committed transaction
        $T_2$ such that $T_2$ has read some object (maybe via a predicate)
        modified by $T_1$.
      </li>
      <li>
        <strong>G1b: Intermediate Reads</strong> A history $H$ shows phenomenon
        G1b if it contains a committed transaction $T_2$ that has read a
        version of object $x$ (maybe via a predicate) written by transaction
        $T_1$ that was not $T_1$'s final modification of $x$.
      </li>
      <li>
        <strong>G1c: Circular Information Flow</strong> A history $H$ exhibits
        phenomenon G1c if DSG($H$) contains a directed cycle consisting
        entirely of dependency edges.
      </li>
      <li>
        <strong>G1</strong> A history $H$ exhibits phenomenon G1 if it exhibits
        G1a, G1b, or G1c.
      </li>
      <li>
        <strong>G2-item: Item Anti-dependency Cycles.</strong> A history $H$
        exhibits phenomenon G2-item if DSG($H$) contains a directed cycle with
        one or more item anti-dependency edges.
      </li>
      <li>
        <strong>G2: Anti-dependency Cycles.</strong> A history $H$ exhibits
        phenomenon G2 if DSG($H$) contains a directed cycle with one or more
        anti-dependency edges.
      </li>
    </ul>

    <p>
    Adya et al. defines generalized isolation levels using these phenomena:
    </p>

    <ul>
      <li><strong>PL-1</strong> proscribes G0.</li>
      <li><strong>PL-2</strong> proscribes G0 and G1.</li>
      <li><strong>PL-2.99</strong> proscribes G0, G1, and G2-item.</li>
      <li><strong>PL-3</strong> proscribes G0, G1, and G2.</li>
    </ul>

    <p>
    PL-1, PL-2, PL-2.99, and PL-3 correspond roughly to read uncommitted, read
    committed, repeatable read, and serializable. In fact, PL-3 provides
    conflict serializability.
    </p>

    <h2>2017: A Client-Centric Specification</h2>
  </div>

  <script type="text/javascript" src="/blog/js/mathjax_config.js"></script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</body>
</html>
